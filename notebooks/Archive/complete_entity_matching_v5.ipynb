{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook contains ALL 8 matching methods:\n",
    "\n",
    "### String Distance Methods (Cohen et al.):\n",
    "1. **Jaro-Winkler** - Character-level similarity with prefix boost\n",
    "2. **Levenshtein** - Edit distance (insertions, deletions, substitutions)\n",
    "3. **Monge-Elkan** - Token-level matching with Jaro-Winkler base\n",
    "\n",
    "### Token-Based Methods (Cohen et al.):\n",
    "4. **TF-IDF** - Traditional cosine similarity on term frequency vectors\n",
    "5. **Soft-TFIDF** - Fuzzy TF-IDF with approximate token matching (Î¸=0.9)\n",
    "\n",
    "### Embedding Methods:\n",
    "6. **SentenceTransformer** - Local embeddings (all-MiniLM-L6-v2)\n",
    "7. **OpenAI Embeddings** - API embeddings (text-embedding-3-small)\n",
    "\n",
    "### LLM Method:\n",
    "8. **GPT-4o-mini** - LLM-based matching with blocking strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jellyfish in /Users/chibuzor/fuzzy-join-llm/.venv/lib/python3.12/site-packages (1.2.1)\n",
      "Requirement already satisfied: py_stringmatching in /Users/chibuzor/fuzzy-join-llm/.venv/lib/python3.12/site-packages (0.4.6)\n",
      "Requirement already satisfied: sentence-transformers in /Users/chibuzor/fuzzy-join-llm/.venv/lib/python3.12/site-packages (5.1.1)\n",
      "Requirement already satisfied: openai in /Users/chibuzor/fuzzy-join-llm/.venv/lib/python3.12/site-packages (2.2.0)\n",
      "Requirement already satisfied: tqdm in /Users/chibuzor/fuzzy-join-llm/.venv/lib/python3.12/site-packages (4.67.1)\n",
      "Requirement already satisfied: scikit-learn in /Users/chibuzor/fuzzy-join-llm/.venv/lib/python3.12/site-packages (1.7.2)\n",
      "Requirement already satisfied: python-dotenv in /Users/chibuzor/fuzzy-join-llm/.venv/lib/python3.12/site-packages (1.1.1)\n",
      "Requirement already satisfied: numpy<2.0,>=1.7.0 in /Users/chibuzor/fuzzy-join-llm/.venv/lib/python3.12/site-packages (from py_stringmatching) (1.26.4)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /Users/chibuzor/fuzzy-join-llm/.venv/lib/python3.12/site-packages (from sentence-transformers) (4.57.0)\n",
      "Requirement already satisfied: torch>=1.11.0 in /Users/chibuzor/fuzzy-join-llm/.venv/lib/python3.12/site-packages (from sentence-transformers) (2.8.0)\n",
      "Requirement already satisfied: scipy in /Users/chibuzor/fuzzy-join-llm/.venv/lib/python3.12/site-packages (from sentence-transformers) (1.16.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /Users/chibuzor/fuzzy-join-llm/.venv/lib/python3.12/site-packages (from sentence-transformers) (0.35.3)\n",
      "Requirement already satisfied: Pillow in /Users/chibuzor/fuzzy-join-llm/.venv/lib/python3.12/site-packages (from sentence-transformers) (11.3.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /Users/chibuzor/fuzzy-join-llm/.venv/lib/python3.12/site-packages (from sentence-transformers) (4.15.0)\n",
      "Requirement already satisfied: filelock in /Users/chibuzor/fuzzy-join-llm/.venv/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (3.20.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/chibuzor/fuzzy-join-llm/.venv/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/chibuzor/fuzzy-join-llm/.venv/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/chibuzor/fuzzy-join-llm/.venv/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.9.18)\n",
      "Requirement already satisfied: requests in /Users/chibuzor/fuzzy-join-llm/.venv/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /Users/chibuzor/fuzzy-join-llm/.venv/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Users/chibuzor/fuzzy-join-llm/.venv/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.6.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/chibuzor/fuzzy-join-llm/.venv/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.9.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Users/chibuzor/fuzzy-join-llm/.venv/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.10)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/chibuzor/fuzzy-join-llm/.venv/lib/python3.12/site-packages (from openai) (4.11.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/chibuzor/fuzzy-join-llm/.venv/lib/python3.12/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/chibuzor/fuzzy-join-llm/.venv/lib/python3.12/site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/chibuzor/fuzzy-join-llm/.venv/lib/python3.12/site-packages (from openai) (0.11.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/chibuzor/fuzzy-join-llm/.venv/lib/python3.12/site-packages (from openai) (2.11.10)\n",
      "Requirement already satisfied: sniffio in /Users/chibuzor/fuzzy-join-llm/.venv/lib/python3.12/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/chibuzor/fuzzy-join-llm/.venv/lib/python3.12/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in /Users/chibuzor/fuzzy-join-llm/.venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/chibuzor/fuzzy-join-llm/.venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/chibuzor/fuzzy-join-llm/.venv/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/chibuzor/fuzzy-join-llm/.venv/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /Users/chibuzor/fuzzy-join-llm/.venv/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/chibuzor/fuzzy-join-llm/.venv/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (0.4.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/chibuzor/fuzzy-join-llm/.venv/lib/python3.12/site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/chibuzor/fuzzy-join-llm/.venv/lib/python3.12/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: setuptools in /Users/chibuzor/fuzzy-join-llm/.venv/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Users/chibuzor/fuzzy-join-llm/.venv/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx in /Users/chibuzor/fuzzy-join-llm/.venv/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
      "Requirement already satisfied: jinja2 in /Users/chibuzor/fuzzy-join-llm/.venv/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/chibuzor/fuzzy-join-llm/.venv/lib/python3.12/site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/chibuzor/fuzzy-join-llm/.venv/lib/python3.12/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/chibuzor/fuzzy-join-llm/.venv/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/chibuzor/fuzzy-join-llm/.venv/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2.5.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install jellyfish py_stringmatching sentence-transformers openai tqdm scikit-learn python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries Imported Successfully!\n"
     ]
    }
   ],
   "source": [
    "# ðŸ“š IMPORTS\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from typing import Tuple, Optional, Set, Dict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# String matching\n",
    "import jellyfish\n",
    "import py_stringmatching as sm\n",
    "\n",
    "# TF-IDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Embeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# OpenAI\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Display\n",
    "from IPython.display import display\n",
    "\n",
    "print(\"Libraries Imported Successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "- **TEST_MODE = True**: Uses only 100 records \n",
    "- **TEST_MODE = False**: Uses full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API key loaded\n",
      "\n",
      "============================================================\n",
      "FULL MODE ENABLED\n",
      "- Using complete datasets\n",
      "============================================================\n",
      "Dataset: abt-buy\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# TEST MODE: Set to True for quick test (100 records)\n",
    "TEST_MODE = False  # \n",
    "\n",
    "# Dataset selection\n",
    "DATASET_NAME = 'abt-buy'  # Options: 'abt-buy', 'amazon-google', 'dblp-acm', 'dblp-scholar'\n",
    "\n",
    "\n",
    "CONFIG = {\n",
    "    'dataset_name': DATASET_NAME,\n",
    "    \n",
    "    # Abt-Buy paths (using ../ to go up from notebooks/ folder)\n",
    "    'dataset_a': '../data/abt-buy/Abt.csv',\n",
    "    'dataset_b': '../data/abt-buy/Buy.csv',\n",
    "    'dataset_a_ciphered': '../data/abt-buy/Abt_ciphered.csv',\n",
    "    'dataset_b_ciphered': '../data/abt-buy/Buy_ciphered.csv',\n",
    "    'dataset_a_scrambled': '../data/abt-buy/Abt_scrambled.csv',\n",
    "    'dataset_b_scrambled': '../data/abt-buy/Buy_scrambled.csv',\n",
    "    'ground_truth': '../data/abt-buy/abt_buy_perfectMapping.csv',\n",
    "    \n",
    "    # Column names for Abt-Buy\n",
    "    'id_col_a': 'id',\n",
    "    'id_col_b': 'id',\n",
    "    'text_cols': ['name', 'description'],\n",
    "    'gt_col_a': 'idAbt',\n",
    "    'gt_col_b': 'idBuy',\n",
    "}\n",
    "\n",
    "# Amazon-Google:\n",
    "# CONFIG['text_cols'] = ['title', 'description', 'manufacturer']\n",
    "# CONFIG['gt_col_a'] = 'idAmazon'\n",
    "# CONFIG['gt_col_b'] = 'idGoogleProducts'\n",
    "\n",
    "# DBLP-ACM or DBLP-Scholar:\n",
    "# CONFIG['text_cols'] = ['title', 'authors', 'venue']\n",
    "# CONFIG['gt_col_a'] = 'idDBLP'\n",
    "# CONFIG['gt_col_b'] = 'idACM'  # or 'idScholar'\n",
    "\n",
    "\n",
    "# OpenAI API Key\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "if OPENAI_API_KEY:\n",
    "    client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "    print(\"OpenAI API key loaded\")\n",
    "else:\n",
    "    print(\"No OpenAI API key - OpenAI methods will be skipped\")\n",
    "\n",
    "# Display Settings\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "if TEST_MODE:\n",
    "    print(\"TEST MODE ENABLED\")\n",
    "    print(\"- Using only 100 records per dataset\")\n",
    "else:\n",
    "    print(\"FULL MODE ENABLED\")\n",
    "    print(\"- Using complete datasets\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Dataset: {CONFIG['dataset_name']}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets with auto-encoding detection...\n",
      "\n",
      "Loading Original Dataset A...\n",
      "   Loaded with latin-1 encoding\n",
      "Dataset A: 1,081 records\n",
      "\n",
      "Loading Original Dataset B...\n",
      "   Loaded with utf-8 encoding\n",
      "Dataset B: 1,092 records\n",
      "\n",
      "Loading Ciphered Dataset A...\n",
      "   Loaded with latin-1 encoding\n",
      "Ciphered A: 1,081 records\n",
      "\n",
      "Loading Ciphered Dataset B...\n",
      "   Loaded with utf-8 encoding\n",
      "Ciphered B: 1,092 records\n",
      "\n",
      "Loading Scrambled Dataset A...\n",
      "   Loaded with latin-1 encoding\n",
      "Scrambled A: 1,081 records\n",
      "\n",
      "Loading Scrambled Dataset B...\n",
      "   Loaded with utf-8 encoding\n",
      "Scrambled B: 1,092 records\n",
      "\n",
      "Loading Ground Truth...\n",
      "   Loaded with utf-8 encoding\n",
      "\n",
      "Ground Truth: 1,097 known matches\n",
      "\n",
      "Data Preview:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>description</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>552</td>\n",
       "      <td>Sony Turntable - PSLX350H</td>\n",
       "      <td>Sony Turntable - PSLX350H/ Belt Drive System/ ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>580</td>\n",
       "      <td>Bose Acoustimass 5 Series III Speaker System -...</td>\n",
       "      <td>Bose Acoustimass 5 Series III Speaker System -...</td>\n",
       "      <td>$399.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4696</td>\n",
       "      <td>Sony Switcher - SBV40S</td>\n",
       "      <td>Sony Switcher - SBV40S/ Eliminates Disconnecti...</td>\n",
       "      <td>$49.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5644</td>\n",
       "      <td>Sony 5 Disc CD Player - CDPCE375</td>\n",
       "      <td>Sony 5 Disc CD Player- CDPCE375/ 5 Disc Change...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6284</td>\n",
       "      <td>Bose 27028 161 Bookshelf Pair Speakers In Whit...</td>\n",
       "      <td>Bose 161 Bookshelf Speakers In White - 161WH/ ...</td>\n",
       "      <td>$158.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id                                               name  \\\n",
       "0   552                          Sony Turntable - PSLX350H   \n",
       "1   580  Bose Acoustimass 5 Series III Speaker System -...   \n",
       "2  4696                             Sony Switcher - SBV40S   \n",
       "3  5644                   Sony 5 Disc CD Player - CDPCE375   \n",
       "4  6284  Bose 27028 161 Bookshelf Pair Speakers In Whit...   \n",
       "\n",
       "                                         description    price  \n",
       "0  Sony Turntable - PSLX350H/ Belt Drive System/ ...      NaN  \n",
       "1  Bose Acoustimass 5 Series III Speaker System -...  $399.00  \n",
       "2  Sony Switcher - SBV40S/ Eliminates Disconnecti...   $49.00  \n",
       "3  Sony 5 Disc CD Player- CDPCE375/ 5 Disc Change...      NaN  \n",
       "4  Bose 161 Bookshelf Speakers In White - 161WH/ ...  $158.00  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def load_csv_with_encoding(filepath):\n",
    "    \"\"\"\n",
    "    Load CSV with automatic encoding detection.\n",
    "    Tries UTF-8, Latin-1, Windows-1252, and ISO-8859-1.\n",
    "    \"\"\"\n",
    "    encodings = ['utf-8', 'latin-1', 'windows-1252', 'iso-8859-1']\n",
    "    \n",
    "    for encoding in encodings:\n",
    "        try:\n",
    "            df = pd.read_csv(filepath, encoding=encoding)\n",
    "            print(f\"   Loaded with {encoding} encoding\")\n",
    "            return df\n",
    "        except (UnicodeDecodeError, UnicodeError):\n",
    "            continue\n",
    "    \n",
    "    # If all fail, raise error\n",
    "    raise ValueError(f\"Could not read {filepath} with any encoding: {encodings}\")\n",
    "\n",
    "print(\"Loading datasets with auto-encoding detection...\\n\")\n",
    "\n",
    "try:\n",
    "    # Load original datasets\n",
    "    print(\"Loading Original Dataset A...\")\n",
    "    df_a = load_csv_with_encoding(CONFIG['dataset_a'])\n",
    "    print(f\"Dataset A: {len(df_a):,} records\\n\")\n",
    "    \n",
    "    print(\"Loading Original Dataset B...\")\n",
    "    df_b = load_csv_with_encoding(CONFIG['dataset_b'])\n",
    "    print(f\"Dataset B: {len(df_b):,} records\\n\")\n",
    "    \n",
    "    # Load ciphered datasets\n",
    "    print(\"Loading Ciphered Dataset A...\")\n",
    "    df_a_ciphered = load_csv_with_encoding(CONFIG['dataset_a_ciphered'])\n",
    "    print(f\"Ciphered A: {len(df_a_ciphered):,} records\\n\")\n",
    "    \n",
    "    print(\"Loading Ciphered Dataset B...\")\n",
    "    df_b_ciphered = load_csv_with_encoding(CONFIG['dataset_b_ciphered'])\n",
    "    print(f\"Ciphered B: {len(df_b_ciphered):,} records\\n\")\n",
    "    \n",
    "    # Load scrambled datasets\n",
    "    print(\"Loading Scrambled Dataset A...\")\n",
    "    df_a_scrambled = load_csv_with_encoding(CONFIG['dataset_a_scrambled'])\n",
    "    print(f\"Scrambled A: {len(df_a_scrambled):,} records\\n\")\n",
    "    \n",
    "    print(\"Loading Scrambled Dataset B...\")\n",
    "    df_b_scrambled = load_csv_with_encoding(CONFIG['dataset_b_scrambled'])\n",
    "    print(f\"Scrambled B: {len(df_b_scrambled):,} records\\n\")\n",
    "    \n",
    "    # Load ground truth\n",
    "    print(\"Loading Ground Truth...\")\n",
    "    ground_truth = load_csv_with_encoding(CONFIG['ground_truth'])\n",
    "    print(f\"\\nGround Truth: {len(ground_truth):,} known matches\")\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"\\nERROR: Could not find file!\")  \n",
    "    raise\n",
    "\n",
    "# ============================================\n",
    "# APPLY TEST MODE\n",
    "# ============================================\n",
    "\n",
    "if TEST_MODE:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"TEST MODE: Reducing to 100 records per dataset\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    df_a = df_a.head(100)\n",
    "    df_b = df_b.head(100)\n",
    "    df_a_ciphered = df_a_ciphered.head(100)\n",
    "    df_b_ciphered = df_b_ciphered.head(100)\n",
    "    df_a_scrambled = df_a_scrambled.head(100)\n",
    "    df_b_scrambled = df_b_scrambled.head(100)\n",
    "    \n",
    "    print(f\" Reduced to {len(df_a)} Ã— {len(df_b)} = {len(df_a) * len(df_b):,} comparisons\")\n",
    "    print(f\"   (Original would be {1081 * 1092:,} comparisons)\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "# Preview\n",
    "print(\"\\nData Preview:\")\n",
    "display(df_a.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing...\n",
      "\n",
      "All datasets preprocessed\n"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "1. Handle missing values\n",
    "2. Convert to lowercase\n",
    "3. Combine multiple columns into one 'text' column\n",
    "\"\"\"\n",
    "\n",
    "def preprocess_text(df, text_columns):\n",
    "    \"\"\"\n",
    "    Preprocess text for matching.\n",
    "    \"\"\"\n",
    "    for col in text_columns:\n",
    "        df[col] = df[col].fillna('')\n",
    "    \n",
    "    df['text'] = df[text_columns].apply(\n",
    "        lambda row: ' '.join(row.values.astype(str)).lower().strip(),\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    return df\n",
    "\n",
    "print(\"Preprocessing...\\n\")\n",
    "\n",
    "df_a = preprocess_text(df_a, CONFIG['text_cols'])\n",
    "df_b = preprocess_text(df_b, CONFIG['text_cols'])\n",
    "df_a_ciphered = preprocess_text(df_a_ciphered, CONFIG['text_cols'])\n",
    "df_b_ciphered = preprocess_text(df_b_ciphered, CONFIG['text_cols'])\n",
    "df_a_scrambled = preprocess_text(df_a_scrambled, CONFIG['text_cols'])\n",
    "df_b_scrambled = preprocess_text(df_b_scrambled, CONFIG['text_cols'])\n",
    "\n",
    "print(\"All datasets preprocessed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Ground Truth Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth: 1,097 true matches\n"
     ]
    }
   ],
   "source": [
    "true_matches = set(\n",
    "    zip(\n",
    "        ground_truth[CONFIG['gt_col_a']].astype(str),\n",
    "        ground_truth[CONFIG['gt_col_b']].astype(str)\n",
    "    )\n",
    ")\n",
    "\n",
    "print(f\"Ground truth: {len(true_matches):,} true matches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Matching Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jaro-Winkler ready\n",
      "Levenshtein ready\n",
      "Monge-Elkan ready\n",
      "Soft-TFIDF ready\n",
      "TF-IDF ready\n",
      "Sentence Transformer matching ready\n",
      "OpenaAI Embedding matching ready\n",
      "LLM matching ready\n"
     ]
    }
   ],
   "source": [
    "# ==================================================\n",
    "# METHOD 1: Jaro-Winkler\n",
    "# ==================================================\n",
    "\n",
    "def jaro_winkler_similarity(str1, str2):\n",
    "    \"\"\"Character-level similarity with prefix boost.\"\"\"\n",
    "    return jellyfish.jaro_winkler_similarity(str1, str2)\n",
    "\n",
    "print(\"Jaro-Winkler ready\")\n",
    "\n",
    "# ==================================================\n",
    "# METHOD 2: Levenshtein\n",
    "# ==================================================\n",
    "\n",
    "def levenshtein_similarity(str1, str2):\n",
    "    \"\"\"Edit distance normalized to 0-1.\"\"\"\n",
    "    distance = jellyfish.levenshtein_distance(str1, str2)\n",
    "    max_len = max(len(str1), len(str2))\n",
    "    if max_len == 0:\n",
    "        return 1.0\n",
    "    return 1.0 - (distance / max_len)\n",
    "\n",
    "print(\"Levenshtein ready\")\n",
    "\n",
    "# ==================================================\n",
    "# METHOD 3: Monge-Elkan\n",
    "# ==================================================\n",
    "\n",
    "def monge_elkan_similarity(str1, str2):\n",
    "    \"\"\"Token-level matching with Jaro-Winkler base.\"\"\"\n",
    "    tokens_a = str1.split()\n",
    "    tokens_b = str2.split()\n",
    "    \n",
    "    if len(tokens_a) == 0 or len(tokens_b) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    sum_best = 0.0\n",
    "    for token_a in tokens_a:\n",
    "        best = max([jaro_winkler_similarity(token_a, token_b) for token_b in tokens_b])\n",
    "        sum_best += best\n",
    "    \n",
    "    return sum_best / len(tokens_a)\n",
    "\n",
    "print(\"Monge-Elkan ready\")\n",
    "\n",
    "# ==================================================\n",
    "# METHOD 4: Soft-TFIDF\n",
    "# ==================================================\n",
    "\n",
    "def soft_tfidf_similarity(str1, str2, theta=0.9):\n",
    "    \"\"\"\n",
    "    Soft-TFIDF with Jaro-Winkler and Î¸=0.9 (Cohen paper).\n",
    "    Simplified version - uses token matching.\n",
    "    \"\"\"\n",
    "    tokens_a = str1.split()\n",
    "    tokens_b = str2.split()\n",
    "    \n",
    "    if len(tokens_a) == 0 or len(tokens_b) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    score = 0.0\n",
    "    for token_a in tokens_a:\n",
    "        best_match = max([jaro_winkler_similarity(token_a, token_b) for token_b in tokens_b])\n",
    "        if best_match >= theta:\n",
    "            score += best_match\n",
    "    \n",
    "    return score / len(tokens_a) if len(tokens_a) > 0 else 0.0\n",
    "\n",
    "print(\"Soft-TFIDF ready\")\n",
    "\n",
    "\n",
    "# ==================================================\n",
    "# METHOD 5: TF-IDF\n",
    "# ==================================================\n",
    "\n",
    "def tfidf_matching(df_a, df_b, threshold, id_col_a, id_col_b):\n",
    "    \"\"\"\n",
    "    TF-IDF cosine similarity with Top-1 matching.\n",
    "    \"\"\"\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    all_texts = pd.concat([df_a['text'], df_b['text']])\n",
    "    vectorizer.fit(all_texts)\n",
    "    \n",
    "    vectors_a = vectorizer.transform(df_a['text'])\n",
    "    vectors_b = vectorizer.transform(df_b['text'])\n",
    "    similarity_matrix = cosine_similarity(vectors_a, vectors_b)\n",
    "    \n",
    "    predicted_matches = set()\n",
    "    for idx_a in range(len(df_a)):\n",
    "        best_idx_b = np.argmax(similarity_matrix[idx_a])\n",
    "        best_sim = similarity_matrix[idx_a, best_idx_b]\n",
    "        \n",
    "        if best_sim >= threshold:\n",
    "            id_a = str(df_a.iloc[idx_a][id_col_a])\n",
    "            id_b = str(df_b.iloc[best_idx_b][id_col_b])\n",
    "            predicted_matches.add((id_a, id_b))\n",
    "    \n",
    "    return predicted_matches\n",
    "\n",
    "print(\"TF-IDF ready\")\n",
    "\n",
    "# ==================================================\n",
    "# METHOD 6: SentenceTransformer\n",
    "# ==================================================\n",
    "\n",
    "sentence_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "def sentence_transformer_matching(df_a, df_b, threshold, id_col_a, id_col_b):\n",
    "    \"\"\"\n",
    "    SentenceTransformer embedding matching.\n",
    "    \"\"\"\n",
    "    embeddings_a = sentence_model.encode(df_a['text'].tolist(), show_progress_bar=False)\n",
    "    embeddings_b = sentence_model.encode(df_b['text'].tolist(), show_progress_bar=False)\n",
    "    similarity_matrix = cosine_similarity(embeddings_a, embeddings_b)\n",
    "    \n",
    "    predicted_matches = set()\n",
    "    for idx_a in range(len(df_a)):\n",
    "        best_idx_b = np.argmax(similarity_matrix[idx_a])\n",
    "        best_sim = similarity_matrix[idx_a, best_idx_b]\n",
    "        \n",
    "        if best_sim >= threshold:\n",
    "            id_a = str(df_a.iloc[idx_a][id_col_a])\n",
    "            id_b = str(df_b.iloc[best_idx_b][id_col_b])\n",
    "            predicted_matches.add((id_a, id_b))\n",
    "    \n",
    "    return predicted_matches\n",
    "\n",
    "print(\"Sentence Transformer matching ready\")\n",
    "\n",
    "# ==================================================\n",
    "# METHOD 7: OpenAI Embedding Method\n",
    "# ==================================================\n",
    "\n",
    "if OPENAI_API_KEY:\n",
    "    \n",
    "    def get_openai_embeddings(texts, model=\"text-embedding-3-small\"):\n",
    "        \"\"\"Get OpenAI embeddings in batches.\"\"\"\n",
    "        embeddings = []\n",
    "        batch_size = 100\n",
    "        \n",
    "        for i in tqdm(range(0, len(texts), batch_size), desc=\"OpenAI embeddings\"):\n",
    "            batch = texts[i:i+batch_size]\n",
    "            response = client.embeddings.create(model=model, input=batch)\n",
    "            batch_embeddings = [item.embedding for item in response.data]\n",
    "            embeddings.extend(batch_embeddings)\n",
    "            time.sleep(0.1)\n",
    "        \n",
    "        return np.array(embeddings)\n",
    "    \n",
    "    def openai_embedding_matching(df_a, df_b, threshold, id_col_a, id_col_b):\n",
    "        \"\"\"OpenAI embedding matching.\"\"\"\n",
    "        embeddings_a = get_openai_embeddings(df_a['text'].tolist())\n",
    "        embeddings_b = get_openai_embeddings(df_b['text'].tolist())\n",
    "        similarity_matrix = cosine_similarity(embeddings_a, embeddings_b)\n",
    "        \n",
    "        predicted_matches = set()\n",
    "        for idx_a in range(len(df_a)):\n",
    "            best_idx_b = np.argmax(similarity_matrix[idx_a])\n",
    "            best_sim = similarity_matrix[idx_a, best_idx_b]\n",
    "            \n",
    "            if best_sim >= threshold:\n",
    "                id_a = str(df_a.iloc[idx_a][id_col_a])\n",
    "                id_b = str(df_b.iloc[best_idx_b][id_col_b])\n",
    "                predicted_matches.add((id_a, id_b))\n",
    "        \n",
    "        return predicted_matches\n",
    "    \n",
    "    print(\"OpenaAI Embedding matching ready\")\n",
    "    \n",
    "# ==================================================\n",
    "\n",
    "# ==================================================\n",
    "# METHOD 8: LLM Matching Method\n",
    "# ==================================================\n",
    "\n",
    "def jaro_winkler_similarity(s1: str, s2: str) -> float:\n",
    "    \"\"\"Calculate Jaro-Winkler similarity between two strings.\"\"\"\n",
    "    if not s1 or not s2:\n",
    "        return 0.0\n",
    "    \n",
    "    s1, s2 = s1.lower(), s2.lower()\n",
    "    \n",
    "    if s1 == s2:\n",
    "        return 1.0\n",
    "    \n",
    "    len1, len2 = len(s1), len(s2)\n",
    "    max_dist = max(len1, len2) // 2 - 1\n",
    "    \n",
    "    if max_dist < 0:\n",
    "        max_dist = 0\n",
    "    \n",
    "    s1_matches = [False] * len1\n",
    "    s2_matches = [False] * len2\n",
    "    matches = 0\n",
    "    \n",
    "    for i in range(len1):\n",
    "        start = max(0, i - max_dist)\n",
    "        end = min(i + max_dist + 1, len2)\n",
    "        \n",
    "        for j in range(start, end):\n",
    "            if s2_matches[j] or s1[i] != s2[j]:\n",
    "                continue\n",
    "            s1_matches[i] = True\n",
    "            s2_matches[j] = True\n",
    "            matches += 1\n",
    "            break\n",
    "    \n",
    "    if matches == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    transpositions = 0\n",
    "    k = 0\n",
    "    for i in range(len1):\n",
    "        if not s1_matches[i]:\n",
    "            continue\n",
    "        while not s2_matches[k]:\n",
    "            k += 1\n",
    "        if s1[i] != s2[k]:\n",
    "            transpositions += 1\n",
    "        k += 1\n",
    "    \n",
    "    jaro = (matches / len1 + matches / len2 + (matches - transpositions / 2) / matches) / 3\n",
    "    \n",
    "    prefix = 0\n",
    "    for i in range(min(len1, len2, 4)):\n",
    "        if s1[i] == s2[i]:\n",
    "            prefix += 1\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    return jaro + prefix * 0.1 * (1 - jaro)\n",
    "\n",
    "\n",
    "\n",
    "def llm_match_improved(record_a: pd.Series, \n",
    "                       df_b: pd.DataFrame, \n",
    "                       id_col_b: str,\n",
    "                       client,\n",
    "                       top_k: int = 20,\n",
    "                       blocking_threshold: float = 0.1,\n",
    "                       max_text_length: int = 500) -> Tuple[Optional[str], float, float, str]:\n",
    "    \"\"\"\n",
    "    Improved LLM matching with optimizations.\n",
    "    \n",
    "    Args:\n",
    "        record_a: Record to match from dataset A\n",
    "        df_b: Dataset B to search for matches\n",
    "        id_col_b: ID column name in dataset B\n",
    "        client: OpenAI client\n",
    "        top_k: Number of candidates to show LLM (default: 20)\n",
    "        blocking_threshold: Minimum Jaro-Winkler to consider (default: 0.1)\n",
    "        max_text_length: Maximum text length to send to LLM (default: 500, 0 = no limit)\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (matched_id, confidence_score, cost, reasoning)\n",
    "    \"\"\"\n",
    "    text_a = record_a['text']\n",
    "    \n",
    "    # IMPROVEMENT 1: More permissive blocking (0.1 instead of 0.3)\n",
    "    # Find top-K candidates using Jaro-Winkler\n",
    "    similarities = []\n",
    "    for idx_b, row_b in df_b.iterrows():\n",
    "        sim = jaro_winkler_similarity(text_a, row_b['text'])\n",
    "        similarities.append((idx_b, sim, row_b[id_col_b], row_b['text']))\n",
    "    \n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # IMPROVEMENT 2: Increased top-k (20 instead of 10)\n",
    "    top_candidates = similarities[:top_k]\n",
    "    \n",
    "    # More permissive filtering\n",
    "    if len(top_candidates) == 0 or top_candidates[0][1] < blocking_threshold:\n",
    "        return None, 0.0, 0.0, f\"Best JW similarity {top_candidates[0][1] if top_candidates else 0:.3f} below threshold {blocking_threshold}\"\n",
    "    \n",
    "    # IMPROVEMENT 3: No truncation (or higher limit)\n",
    "    # Build candidate list with full text (or limited to max_text_length)\n",
    "    candidates_list = []\n",
    "    for i, cand in enumerate(top_candidates):\n",
    "        text = cand[3]\n",
    "        if max_text_length > 0 and len(text) > max_text_length:\n",
    "            text = text[:max_text_length] + \"...\"\n",
    "        candidates_list.append(f\"[{i+1}] {text}\")\n",
    "    \n",
    "    candidates_text = \"\\n\\n\".join(candidates_list)\n",
    "    \n",
    "    # Truncate query text if needed\n",
    "    query_text = text_a\n",
    "    if max_text_length > 0 and len(query_text) > max_text_length:\n",
    "        query_text = query_text[:max_text_length] + \"...\"\n",
    "    \n",
    "    # IMPROVEMENT 4: Enhanced prompt with context and guidance\n",
    "    prompt = f\"\"\"You are an expert at entity matching for product databases. Your task is to determine if any candidate record matches the query record, considering they come from different sources with varying descriptions.\n",
    "\n",
    "QUERY RECORD:\n",
    "{query_text}\n",
    "\n",
    "CANDIDATE RECORDS:\n",
    "{candidates_text}\n",
    "\n",
    "MATCHING GUIDELINES:\n",
    "- Products are the SAME if they refer to the same model/item, even with different descriptions\n",
    "- Consider: brand names, model numbers, key specifications\n",
    "- Account for: abbreviations (e.g., \"PS5\" = \"PlayStation 5\"), different word orders, extra/missing details\n",
    "- Be strict about: core product identity (don't match PS5 with PS4, iPad Pro 11\" with iPad Pro 12.9\")\n",
    "- Different bundles/configurations of the same base product may or may not match depending on significance\n",
    "\n",
    "RESPONSE FORMAT:\n",
    "If a match exists, respond with:\n",
    "Match: [number]\n",
    "Confidence: [0.0 to 1.0]\n",
    "Reasoning: [brief explanation]\n",
    "\n",
    "If no match exists, respond with:\n",
    "Match: 0\n",
    "Confidence: [0.0 to 1.0]\n",
    "Reasoning: [why no candidates match]\n",
    "\n",
    "Your response:\"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0\n",
    "        )\n",
    "        \n",
    "        answer = response.choices[0].message.content.strip()\n",
    "        tokens = response.usage.total_tokens\n",
    "        cost = (tokens / 1_000_000) * 0.15\n",
    "        \n",
    "        # IMPROVEMENT 5: Parse confidence score\n",
    "        match_num = 0\n",
    "        confidence = 0.0\n",
    "        reasoning = answer\n",
    "        \n",
    "        # Parse the response\n",
    "        lines = answer.split('\\n')\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if line.startswith('Match:'):\n",
    "                try:\n",
    "                    match_num = int(''.join(c for c in line.split('Match:')[1] if c.isdigit()))\n",
    "                except:\n",
    "                    pass\n",
    "            elif line.startswith('Confidence:'):\n",
    "                try:\n",
    "                    conf_str = line.split('Confidence:')[1].strip()\n",
    "                    confidence = float(''.join(c for c in conf_str if c.isdigit() or c == '.'))\n",
    "                    # Ensure confidence is between 0 and 1\n",
    "                    if confidence > 1.0:\n",
    "                        confidence = confidence / 100.0\n",
    "                except:\n",
    "                    pass\n",
    "            elif line.startswith('Reasoning:'):\n",
    "                reasoning = line.split('Reasoning:')[1].strip()\n",
    "        \n",
    "        # Return match if valid choice\n",
    "        if match_num > 0 and match_num <= len(top_candidates):\n",
    "            matched_id = top_candidates[match_num - 1][2]\n",
    "            return matched_id, confidence, cost, reasoning\n",
    "        \n",
    "        return None, confidence, cost, reasoning\n",
    "        \n",
    "    except Exception as e:\n",
    "        return None, 0.0, 0.0, f\"Error: {str(e)}\"\n",
    "\n",
    "\n",
    "\n",
    "def llm_matching_improved(df_a: pd.DataFrame,\n",
    "                          df_b: pd.DataFrame, \n",
    "                          id_col_a: str,\n",
    "                          id_col_b: str,\n",
    "                          client,\n",
    "                          confidence_threshold: float = 0.5,\n",
    "                          top_k: int = 20,\n",
    "                          blocking_threshold: float = 0.1,\n",
    "                          max_text_length: int = 500,\n",
    "                          save_details: bool = False) -> Tuple[Set[Tuple[str, str]], float, Optional[pd.DataFrame]]:\n",
    "    \"\"\"\n",
    "    Full improved LLM matching with confidence threshold optimization.\n",
    "    \n",
    "    Args:\n",
    "        df_a: Dataset A\n",
    "        df_b: Dataset B\n",
    "        id_col_a: ID column in dataset A\n",
    "        id_col_b: ID column in dataset B\n",
    "        client: OpenAI client\n",
    "        confidence_threshold: Minimum confidence to predict match (default: 0.5)\n",
    "        top_k: Number of candidates per query (default: 20)\n",
    "        blocking_threshold: Minimum JW similarity (default: 0.1)\n",
    "        max_text_length: Max text length (default: 500, 0 = unlimited)\n",
    "        save_details: Whether to return detailed results DataFrame\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (predicted_matches, total_cost, details_df)\n",
    "    \"\"\"\n",
    "    predicted_matches = set()\n",
    "    total_cost = 0.0\n",
    "    details = []\n",
    "    \n",
    "    print(f\"\\nðŸš€ Running IMPROVED LLM matching with:\")\n",
    "    print(f\"   â€¢ Top-K: {top_k} (baseline: 10)\")\n",
    "    print(f\"   â€¢ Blocking threshold: {blocking_threshold} (baseline: 0.3)\")\n",
    "    print(f\"   â€¢ Max text length: {max_text_length} chars (baseline: 100)\")\n",
    "    print(f\"   â€¢ Confidence threshold: {confidence_threshold}\")\n",
    "    print(f\"   â€¢ Enhanced prompt: âœ“\")\n",
    "    print()\n",
    "    \n",
    "    for idx_a, row_a in tqdm(df_a.iterrows(), total=len(df_a), desc=\"Improved LLM matching\"):\n",
    "        matched_id, confidence, cost, reasoning = llm_match_improved(\n",
    "            row_a, df_b, id_col_b, client, top_k, blocking_threshold, max_text_length\n",
    "        )\n",
    "        \n",
    "        total_cost += cost\n",
    "        \n",
    "        # IMPROVEMENT 5: Use confidence threshold\n",
    "        if matched_id and confidence >= confidence_threshold:\n",
    "            predicted_matches.add((str(row_a[id_col_a]), str(matched_id)))\n",
    "        \n",
    "        if save_details:\n",
    "            details.append({\n",
    "                'id_a': str(row_a[id_col_a]),\n",
    "                'text_a': row_a['text'][:100] + \"...\",\n",
    "                'matched_id_b': str(matched_id) if matched_id else None,\n",
    "                'confidence': confidence,\n",
    "                'cost': cost,\n",
    "                'reasoning': reasoning[:200] if reasoning else None,\n",
    "                'predicted': matched_id is not None and confidence >= confidence_threshold\n",
    "            })\n",
    "    \n",
    "    details_df = pd.DataFrame(details) if save_details else None\n",
    "    \n",
    "    return predicted_matches, total_cost, details_df\n",
    "\n",
    "\n",
    "\n",
    "def optimize_llm_threshold(df_a: pd.DataFrame,\n",
    "                           df_b: pd.DataFrame,\n",
    "                           true_matches: Set[Tuple[str, str]],\n",
    "                           id_col_a: str,\n",
    "                           id_col_b: str,\n",
    "                           client,\n",
    "                           details_df: pd.DataFrame,\n",
    "                           method_name: str = \"Improved-LLM\") -> Dict:\n",
    "    \"\"\"\n",
    "    Optimize confidence threshold for improved LLM matching.\n",
    "    \n",
    "    Since we already ran the LLM and have confidence scores, we can\n",
    "    optimize the threshold without re-running (saves cost!).\n",
    "    \n",
    "    Args:\n",
    "        df_a, df_b: Datasets\n",
    "        true_matches: Ground truth matches\n",
    "        id_col_a, id_col_b: ID columns\n",
    "        client: OpenAI client (not used here, but kept for consistency)\n",
    "        details_df: DataFrame with confidence scores from previous run\n",
    "        method_name: Name for display\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with best threshold and metrics\n",
    "    \"\"\"\n",
    "    print(f\"\\nðŸ” Optimizing confidence threshold for {method_name}...\")\n",
    "    print(\"(Using cached LLM responses - no additional API calls!)\")\n",
    "    \n",
    "    thresholds = np.arange(0.0, 1.01, 0.05)\n",
    "    best_threshold = 0.5\n",
    "    best_f1 = 0.0\n",
    "    results = []\n",
    "    \n",
    "    total_pairs = len(df_a) * len(df_b)\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        # Build predicted matches using this threshold\n",
    "        predicted = set()\n",
    "        for _, row in details_df.iterrows():\n",
    "            if row['matched_id_b'] is not None and row['confidence'] >= threshold:\n",
    "                predicted.add((row['id_a'], row['matched_id_b']))\n",
    "        \n",
    "        # Calculate metrics\n",
    "        tp = len(predicted & true_matches)\n",
    "        fp = len(predicted - true_matches)\n",
    "        fn = len(true_matches - predicted)\n",
    "        tn = total_pairs - tp - fp - fn\n",
    "        \n",
    "        precision = tp / len(predicted) if len(predicted) > 0 else 0.0\n",
    "        recall = tp / len(true_matches) if len(true_matches) > 0 else 0.0\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "        \n",
    "        results.append({\n",
    "            'threshold': threshold,\n",
    "            'f1': f1,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'predicted_count': len(predicted)\n",
    "        })\n",
    "        \n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = threshold\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    best_row = results_df[results_df['threshold'] == best_threshold].iloc[0]\n",
    "    \n",
    "    print(f\"\\nâœ… Best threshold: {best_threshold:.2f}\")\n",
    "    print(f\"   F1: {best_row['f1']:.3f}\")\n",
    "    print(f\"   Precision: {best_row['precision']:.3f}\")\n",
    "    print(f\"   Recall: {best_row['recall']:.3f}\")\n",
    "    print(f\"   Predicted matches: {best_row['predicted_count']:.0f}\")\n",
    "    \n",
    "    return {\n",
    "        'best_threshold': best_threshold,\n",
    "        'best_f1': best_f1,\n",
    "        'best_precision': best_row['precision'],\n",
    "        'best_recall': best_row['recall'],\n",
    "        'threshold_results': results_df\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"LLM matching ready\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top-1 Matching & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching and evaluation functions ready!\n"
     ]
    }
   ],
   "source": [
    "def top1_matching(df_a, df_b, similarity_function, threshold, id_col_a, id_col_b):\n",
    "    \"\"\"\n",
    "    Top-1 matching strategy.\n",
    "    \"\"\"\n",
    "    predicted_matches = set()\n",
    "    \n",
    "    for idx_a, row_a in tqdm(df_a.iterrows(), total=len(df_a), desc=\"Matching\"):\n",
    "        best_sim = 0.0\n",
    "        best_match_id = None\n",
    "        \n",
    "        for idx_b, row_b in df_b.iterrows():\n",
    "            sim = similarity_function(row_a['text'], row_b['text'])\n",
    "            if sim > best_sim:\n",
    "                best_sim = sim\n",
    "                best_match_id = row_b[id_col_b]\n",
    "        \n",
    "        if best_sim >= threshold:\n",
    "            predicted_matches.add((str(row_a[id_col_a]), str(best_match_id)))\n",
    "    \n",
    "    return predicted_matches\n",
    "\n",
    "def calculate_metrics(predicted_matches, true_matches, total_possible_pairs):\n",
    "    \"\"\"\n",
    "    Calculate evaluation metrics.\n",
    "    \"\"\"\n",
    "    tp = len(predicted_matches & true_matches)\n",
    "    fp = len(predicted_matches - true_matches)\n",
    "    fn = len(true_matches - predicted_matches)\n",
    "    tn = total_possible_pairs - tp - fp - fn\n",
    "    \n",
    "    precision = tp / len(predicted_matches) if len(predicted_matches) > 0 else 0.0\n",
    "    recall = tp / len(true_matches) if len(true_matches) > 0 else 0.0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "    accuracy = (tp + tn) / total_possible_pairs if total_possible_pairs > 0 else 0.0\n",
    "    \n",
    "    return {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'accuracy': accuracy,\n",
    "        'true_positives': tp,\n",
    "        'false_positives': fp,\n",
    "        'false_negatives': fn\n",
    "    }\n",
    "\n",
    "def find_best_threshold(df_a, df_b, similarity_function, true_matches,\n",
    "                        id_col_a, id_col_b, method_name):\n",
    "    \"\"\"\n",
    "    Find optimal threshold.\n",
    "    \"\"\"\n",
    "    print(f\"\\nðŸ” Optimizing threshold for {method_name}...\")\n",
    "    \n",
    "    thresholds = np.arange(0.50, 0.96, 0.05)\n",
    "    best_threshold = 0.0\n",
    "    best_f1 = 0.0\n",
    "    total_pairs = len(df_a) * len(df_b)\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        predicted = top1_matching(df_a, df_b, similarity_function,\n",
    "                                  threshold, id_col_a, id_col_b)\n",
    "        metrics = calculate_metrics(predicted, true_matches, total_pairs)\n",
    "        \n",
    "        if metrics['f1'] > best_f1:\n",
    "            best_f1 = metrics['f1']\n",
    "            best_threshold = threshold\n",
    "        \n",
    "        print(f\"  Threshold {threshold:.2f}: F1={metrics['f1']:.3f}\")\n",
    "    \n",
    "    print(f\"\\nâœ… Best: {best_threshold:.2f} (F1={best_f1:.3f})\")\n",
    "    return best_threshold\n",
    "\n",
    "print(\"Matching and evaluation functions ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run All Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STARTING ALL EXPERIMENTS\n",
      "================================================================================\n",
      "Total experiments: 18\n",
      "   + 6 more with OpenAI methods = 24\n",
      "================================================================================\n",
      "\n",
      "[1] Jaro-Winkler on Original\n",
      "================================================================================\n",
      "\n",
      "ðŸ” Optimizing threshold for Jaro-Winkler...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matching:   1%|          | 13/1081 [00:04<06:41,  2.66it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 35\u001b[39m\n\u001b[32m     31\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m80\u001b[39m)\n\u001b[32m     33\u001b[39m start_time = time.time()\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m best_threshold = \u001b[43mfind_best_threshold\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_a\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_b\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msimilarity_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrue_matches\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m    \u001b[49m\u001b[43mCONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mid_col_a\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mid_col_b\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod_name\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m predicted = top1_matching(\n\u001b[32m     41\u001b[39m     data_a, data_b, similarity_func,\n\u001b[32m     42\u001b[39m     best_threshold, CONFIG[\u001b[33m'\u001b[39m\u001b[33mid_col_a\u001b[39m\u001b[33m'\u001b[39m], CONFIG[\u001b[33m'\u001b[39m\u001b[33mid_col_b\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     43\u001b[39m )\n\u001b[32m     45\u001b[39m metrics = calculate_metrics(predicted, true_matches, \u001b[38;5;28mlen\u001b[39m(data_a) * \u001b[38;5;28mlen\u001b[39m(data_b))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 59\u001b[39m, in \u001b[36mfind_best_threshold\u001b[39m\u001b[34m(df_a, df_b, similarity_function, true_matches, id_col_a, id_col_b, method_name)\u001b[39m\n\u001b[32m     56\u001b[39m total_pairs = \u001b[38;5;28mlen\u001b[39m(df_a) * \u001b[38;5;28mlen\u001b[39m(df_b)\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m threshold \u001b[38;5;129;01min\u001b[39;00m thresholds:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     predicted = \u001b[43mtop1_matching\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_a\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_b\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msimilarity_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[43m                              \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mid_col_a\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mid_col_b\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     61\u001b[39m     metrics = calculate_metrics(predicted, true_matches, total_pairs)\n\u001b[32m     63\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m metrics[\u001b[33m'\u001b[39m\u001b[33mf1\u001b[39m\u001b[33m'\u001b[39m] > best_f1:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36mtop1_matching\u001b[39m\u001b[34m(df_a, df_b, similarity_function, threshold, id_col_a, id_col_b)\u001b[39m\n\u001b[32m      9\u001b[39m best_match_id = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m idx_b, row_b \u001b[38;5;129;01min\u001b[39;00m df_b.iterrows():\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m     sim = \u001b[43msimilarity_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow_a\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtext\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow_b\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtext\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m sim > best_sim:\n\u001b[32m     14\u001b[39m         best_sim = sim\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 202\u001b[39m, in \u001b[36mjaro_winkler_similarity\u001b[39m\u001b[34m(s1, s2)\u001b[39m\n\u001b[32m    200\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(start, end):\n\u001b[32m    201\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m s2_matches[j] \u001b[38;5;129;01mor\u001b[39;00m s1[i] != s2[j]:\n\u001b[32m--> \u001b[39m\u001b[32m202\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m    203\u001b[39m     s1_matches[i] = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    204\u001b[39m     s2_matches[j] = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "all_results = []\n",
    "\n",
    "datasets = {\n",
    "    'Original': (df_a, df_b),\n",
    "    'Ciphered': (df_a_ciphered, df_b_ciphered),\n",
    "    'Scrambled': (df_a_scrambled, df_b_scrambled)\n",
    "}\n",
    "\n",
    "string_methods = {\n",
    "    'Jaro-Winkler': jaro_winkler_similarity,\n",
    "    'Levenshtein': levenshtein_similarity,\n",
    "    'Monge-Elkan': monge_elkan_similarity,\n",
    "    'Soft-TFIDF': soft_tfidf_similarity\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STARTING ALL EXPERIMENTS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Total experiments: {(len(string_methods) + 2) * 3}\")\n",
    "if OPENAI_API_KEY:\n",
    "    print(f\"   + 6 more with OpenAI methods = {(len(string_methods) + 2) * 3 + 6}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "experiment_num = 1\n",
    "\n",
    "# String methods\n",
    "for method_name, similarity_func in string_methods.items():\n",
    "    for dataset_name, (data_a, data_b) in datasets.items():\n",
    "        \n",
    "        print(f\"\\n[{experiment_num}] {method_name} on {dataset_name}\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        best_threshold = find_best_threshold(\n",
    "            data_a, data_b, similarity_func, true_matches,\n",
    "            CONFIG['id_col_a'], CONFIG['id_col_b'], method_name\n",
    "        )\n",
    "        \n",
    "        predicted = top1_matching(\n",
    "            data_a, data_b, similarity_func,\n",
    "            best_threshold, CONFIG['id_col_a'], CONFIG['id_col_b']\n",
    "        )\n",
    "        \n",
    "        metrics = calculate_metrics(predicted, true_matches, len(data_a) * len(data_b))\n",
    "        exec_time = time.time() - start_time\n",
    "        \n",
    "        all_results.append({\n",
    "            'Method': method_name,\n",
    "            'Dataset': dataset_name,\n",
    "            'Threshold': best_threshold,\n",
    "            'F1': metrics['f1'],\n",
    "            'Precision': metrics['precision'],\n",
    "            'Recall': metrics['recall'],\n",
    "            'Accuracy': metrics['accuracy'],\n",
    "            'Time (s)': exec_time,\n",
    "            'Cost ($)': 0.0\n",
    "        })\n",
    "        \n",
    "        print(f\"F1={metrics['f1']:.3f}, Time={exec_time:.1f}s\")\n",
    "        experiment_num += 1\n",
    "\n",
    "# TF-IDF\n",
    "for dataset_name, (data_a, data_b) in datasets.items():\n",
    "    print(f\"\\n[{experiment_num}] TF-IDF on {dataset_name}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    best_threshold = 0.0\n",
    "    best_f1 = 0.0\n",
    "    total_pairs = len(data_a) * len(data_b)\n",
    "    \n",
    "    for threshold in np.arange(0.50, 0.96, 0.05):\n",
    "        predicted = tfidf_matching(data_a, data_b, threshold,\n",
    "                                  CONFIG['id_col_a'], CONFIG['id_col_b'])\n",
    "        metrics = calculate_metrics(predicted, true_matches, total_pairs)\n",
    "        if metrics['f1'] > best_f1:\n",
    "            best_f1 = metrics['f1']\n",
    "            best_threshold = threshold\n",
    "    \n",
    "    predicted = tfidf_matching(data_a, data_b, best_threshold,\n",
    "                              CONFIG['id_col_a'], CONFIG['id_col_b'])\n",
    "    metrics = calculate_metrics(predicted, true_matches, total_pairs)\n",
    "    exec_time = time.time() - start_time\n",
    "    \n",
    "    all_results.append({\n",
    "        'Method': 'TF-IDF',\n",
    "        'Dataset': dataset_name,\n",
    "        'Threshold': best_threshold,\n",
    "        'F1': metrics['f1'],\n",
    "        'Precision': metrics['precision'],\n",
    "        'Recall': metrics['recall'],\n",
    "        'Accuracy': metrics['accuracy'],\n",
    "        'Time (s)': exec_time,\n",
    "        'Cost ($)': 0.0\n",
    "    })\n",
    "    \n",
    "    print(f\"F1={metrics['f1']:.3f}, Time={exec_time:.1f}s\")\n",
    "    experiment_num += 1\n",
    "\n",
    "# SentenceTransformer\n",
    "for dataset_name, (data_a, data_b) in datasets.items():\n",
    "    print(f\"\\n[{experiment_num}] SentenceTransformer on {dataset_name}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    best_threshold = 0.0\n",
    "    best_f1 = 0.0\n",
    "    total_pairs = len(data_a) * len(data_b)\n",
    "    \n",
    "    for threshold in np.arange(0.50, 0.96, 0.05):\n",
    "        predicted = sentence_transformer_matching(data_a, data_b, threshold,\n",
    "                                                 CONFIG['id_col_a'], CONFIG['id_col_b'])\n",
    "        metrics = calculate_metrics(predicted, true_matches, total_pairs)\n",
    "        if metrics['f1'] > best_f1:\n",
    "            best_f1 = metrics['f1']\n",
    "            best_threshold = threshold\n",
    "    \n",
    "    exec_time = time.time() - start_time\n",
    "    \n",
    "    all_results.append({\n",
    "        'Method': 'SentenceTransformer',\n",
    "        'Dataset': dataset_name,\n",
    "        'Threshold': best_threshold,\n",
    "        'F1': best_f1,\n",
    "        'Precision': metrics['precision'],\n",
    "        'Recall': metrics['recall'],\n",
    "        'Accuracy': metrics['accuracy'],\n",
    "        'Time (s)': exec_time,\n",
    "        'Cost ($)': 0.0\n",
    "    })\n",
    "    \n",
    "    print(f\"âœ… F1={best_f1:.3f}, Time={exec_time:.1f}s\")\n",
    "    experiment_num += 1\n",
    "\n",
    "# OpenAI methods \n",
    "if OPENAI_API_KEY:\n",
    "    # OpenAI Embeddings\n",
    "    for dataset_name, (data_a, data_b) in datasets.items():\n",
    "        print(f\"\\n[{experiment_num}] OpenAI Embeddings on {dataset_name}\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        best_threshold = 0.0\n",
    "        best_f1 = 0.0\n",
    "        \n",
    "        for threshold in np.arange(0.50, 0.96, 0.05):\n",
    "            predicted = openai_embedding_matching(data_a, data_b, threshold,\n",
    "                                                 CONFIG['id_col_a'], CONFIG['id_col_b'])\n",
    "            metrics = calculate_metrics(predicted, true_matches, len(data_a) * len(data_b))\n",
    "            if metrics['f1'] > best_f1:\n",
    "                best_f1 = metrics['f1']\n",
    "                best_threshold = threshold\n",
    "        \n",
    "        exec_time = time.time() - start_time\n",
    "        est_cost = (len(data_a) + len(data_b)) * 0.00002\n",
    "        \n",
    "        all_results.append({\n",
    "            'Method': 'OpenAI-Embedding',\n",
    "            'Dataset': dataset_name,\n",
    "            'Threshold': best_threshold,\n",
    "            'F1': best_f1,\n",
    "            'Precision': metrics['precision'],\n",
    "            'Recall': metrics['recall'],\n",
    "            'Accuracy': metrics['accuracy'],\n",
    "            'Time (s)': exec_time,\n",
    "            'Cost ($)': est_cost\n",
    "        })\n",
    "        \n",
    "        print(f\"âœ… F1={best_f1:.3f}, Cost=${est_cost:.4f}\")\n",
    "        experiment_num += 1\n",
    "    \n",
    "    # LLM\n",
    "    for dataset_name, (data_a, data_b) in datasets.items():\n",
    "        print(f\"\\n[{experiment_num}] LLM (GPT-4o-mini) on {dataset_name}\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        predicted, llm_cost, details_df = llm_matching_improved(\n",
    "            data_a, data_b,\n",
    "            CONFIG['id_col_a'], CONFIG['id_col_b'],\n",
    "            client,\n",
    "            confidence_threshold=0.5,\n",
    "            top_k=20,\n",
    "            blocking_threshold=0.1,\n",
    "            max_text_length=500,\n",
    "            save_details=True\n",
    "        )\n",
    "        \n",
    "        # Optimize threshold\n",
    "        optimization_results = optimize_llm_threshold(\n",
    "            data_a, data_b, true_matches,\n",
    "            CONFIG['id_col_a'], CONFIG['id_col_b'],\n",
    "            client, details_df,\n",
    "            f\"Improved-LLM-{dataset_name}\"\n",
    "        )\n",
    "        \n",
    "        best_threshold = optimization_results['best_threshold']\n",
    "        \n",
    "        # Get optimized predictions  \n",
    "        predicted = set()\n",
    "        for _, row in details_df.iterrows():\n",
    "            if row['matched_id_b'] is not None and row['confidence'] >= best_threshold:\n",
    "                predicted.add((row['id_a'], row['matched_id_b']))\n",
    "        \n",
    "        llm_cost = llm_cost  # Keep original cost\n",
    "        \n",
    "        metrics = calculate_metrics(predicted, true_matches, len(data_a) * len(data_b))\n",
    "        exec_time = time.time() - start_time\n",
    "        \n",
    "        all_results.append({\n",
    "            'Method': 'LLM-GPT4o-mini',\n",
    "            'Dataset': dataset_name,\n",
    "            'Threshold': best_threshold,\n",
    "            'F1': metrics['f1'],\n",
    "            'Precision': metrics['precision'],\n",
    "            'Recall': metrics['recall'],\n",
    "            'Accuracy': metrics['accuracy'],\n",
    "            'Time (s)': exec_time,\n",
    "            'Cost ($)': llm_cost\n",
    "        })\n",
    "        \n",
    "        print(f\"âœ… F1={metrics['f1']:.3f}, Cost=${llm_cost:.4f}\")\n",
    "        experiment_num += 1\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ALL EXPERIMENTS COMPLETE!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“Š Step 9: Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m results_df = pd.DataFrame(\u001b[43mall_results\u001b[49m)\n\u001b[32m      2\u001b[39m results_df_sorted = results_df.sort_values(\u001b[33m'\u001b[39m\u001b[33mF1\u001b[39m\u001b[33m'\u001b[39m, ascending=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m100\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'all_results' is not defined"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame(all_results)\n",
    "results_df_sorted = results_df.sort_values('F1', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"ðŸ“Š COMPLETE RESULTS\")\n",
    "print(\"=\"*100)\n",
    "display(results_df_sorted)\n",
    "\n",
    "# Performance drops\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"ðŸ“‰ PERFORMANCE DROP ANALYSIS\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "for method in results_df['Method'].unique():\n",
    "    print(f\"\\n{method}:\")\n",
    "    method_results = results_df[results_df['Method'] == method]\n",
    "    \n",
    "    orig = method_results[method_results['Dataset'] == 'Original']['F1'].values\n",
    "    ciph = method_results[method_results['Dataset'] == 'Ciphered']['F1'].values\n",
    "    scra = method_results[method_results['Dataset'] == 'Scrambled']['F1'].values\n",
    "    \n",
    "    if len(orig) > 0:\n",
    "        print(f\"  Original:   {orig[0]:.3f}\")\n",
    "        if len(ciph) > 0:\n",
    "            drop = ((orig[0] - ciph[0]) / orig[0]) * 100 if orig[0] > 0 else 0\n",
    "            print(f\"  Ciphered:   {ciph[0]:.3f}  (â†“ {drop:.1f}%)\")\n",
    "        if len(scra) > 0:\n",
    "            drop = ((orig[0] - scra[0]) / orig[0]) * 100 if orig[0] > 0 else 0\n",
    "            print(f\"  Scrambled:  {scra[0]:.3f}  (â†“ {drop:.1f}%)\")\n",
    "\n",
    "# Best methods\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"ðŸ† BEST METHOD PER DATASET\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "for dataset in ['Original', 'Ciphered', 'Scrambled']:\n",
    "    dataset_results = results_df[results_df['Dataset'] == dataset]\n",
    "    best = dataset_results.loc[dataset_results['F1'].idxmax()]\n",
    "    print(f\"\\n{dataset}: {best['Method']} (F1={best['F1']:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ’¾ Step 10: Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode_suffix = \"_TEST\" if TEST_MODE else \"_FULL\"\n",
    "output_file = f\"results_{CONFIG['dataset_name']}{mode_suffix}.csv\"\n",
    "\n",
    "results_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"\\nðŸ’¾ Results saved to: {output_file}\")\n",
    "print(f\"\\nâœ… Next steps:\")\n",
    "if TEST_MODE:\n",
    "    print(f\"   1. Review the results above\")\n",
    "    print(f\"   2. If everything looks good, set TEST_MODE = False\")\n",
    "    print(f\"   3. Run the full experiment (~3-4 hours)\")\n",
    "else:\n",
    "    print(f\"   1. Share results with your professor\")\n",
    "    print(f\"   2. Run on other datasets if needed\")\n",
    "    print(f\"   3. Create visualizations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Summary\n",
    "\n",
    "### âœ… What You Have:\n",
    "- Complete implementation of 8 matching methods\n",
    "- Tested on 3 dataset versions (Original, Ciphered, Scrambled)\n",
    "- Full evaluation metrics (F1, Precision, Recall, Accuracy)\n",
    "- Performance drop analysis\n",
    "- Results saved to CSV\n",
    "\n",
    "### ðŸ“Š Key Findings:\n",
    "- Review the results table above\n",
    "- Compare Original vs Ciphered vs Scrambled\n",
    "- Identify which methods are most robust\n",
    "\n",
    "### ðŸš€ Next Steps:\n",
    "1. If TEST_MODE: Review results, then run full experiment\n",
    "2. If FULL_MODE: Share results with professor\n",
    "3. Replicate on other datasets (amazon-google, dblp-acm, dblp-scholar)\n",
    "\n",
    "---\n",
    "\n",
    "**Great work!** ðŸŽ‰"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fuzzy-join-llm",
   "language": "python",
   "name": "fuzzy-join-llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
